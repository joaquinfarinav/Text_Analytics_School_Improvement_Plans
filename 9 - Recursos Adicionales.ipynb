{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b65d3c9b",
   "metadata": {},
   "source": [
    "**Notebook 9: Recursos adicionales**\n",
    "\n",
    "Objetivo: Recopilación de trozos de código de ánalisis NLP que fueron evaluados pero no utilizados en la versión final \n",
    "    del estudio. No obstante, pueder ser de gran utilidad para otras tareas específicas. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "822aa86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from scipy import spatial\n",
    "import spacy\n",
    "import nltk\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.corpora import Dictionary\n",
    "import nltk\n",
    "import spacy\n",
    "from fuzzy_sentences_clustering import look_for_clusters\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, EncoderDecoderModel\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0471edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios [cambiar a directorio propios]\n",
    "pme_path = r'C:\\Users\\JoaquinFarina\\Dropbox\\Team NLP\\Data\\Planes de Mejoramiento Educativo\\InformeFinal'\n",
    "# establecer directorio de trabajo\n",
    "os.chdir(pme_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f020728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar bbdd\n",
    "df_planificacion_implementacion_total = pd.read_csv(r'Datos\\Finales\\Nivel_accion\\df_planificacion_implementacion_total_2018_2022_ACCIONES_FINAL.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2a71512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['agno', 'rbd', 'region', 'deprov', 'comuna', 'dependencia',\n",
       "       'nombre_actividad', 'en_planificacion_anual', 'dimensión_planif',\n",
       "       'subdimensión_planif', 'objetivo_planif', 'estrategia_planif',\n",
       "       'descripción_del_plan_planif', 'fecha_inicio_planif',\n",
       "       'fecha_término_planif', 'programa_asociado_planif',\n",
       "       'nombre_responsable_planif', 'ate_planif', 'planes_planif',\n",
       "       'en_reporte_implementacion', 'dimensión_implem', 'subdimensión_implem',\n",
       "       'objetivo_implem', 'estrategia_implem', 'descripción_del_plan_implem',\n",
       "       'fecha_inicio_implem', 'fecha_término_implem',\n",
       "       'programa_asociado_implem', 'nombre_responsable_implem', 'ate_implem',\n",
       "       'planes_implem', 'monto_subvencion_general', 'monto_sep', 'monto_pie',\n",
       "       'monto_mantenimiento', 'monto_pro_retencion', 'monto_internado',\n",
       "       'monto_reforzamiento', 'monto_feap', 'monto_aporte_municipal',\n",
       "       'monto_total', 'nombre_nivel_ejecucion',\n",
       "       'nombre_justificacion_nivel_ejecucion', 'rural_rbd', 'básica_adultos',\n",
       "       'básica_niños', 'media_hc_adultos', 'media_hc_jóvenes',\n",
       "       'media_tp_y_artística_adultos', 'media_tp_y_artística_jóvenes',\n",
       "       'parvularia', 'parvularia_ind', 'básica_niños_ind',\n",
       "       'media_hc_jóvenes_ind', 'media_tp_y_artística_jóvenes_ind',\n",
       "       'básica_adultos_ind', 'media_hc_adultos_ind',\n",
       "       'media_tp_y_artística_adultos_ind', 'total_niveles',\n",
       "       'cod_total_niveles', 'total_matricula', 'asistencia', 'prioritario_alu',\n",
       "       'idps_am_agno_previo', 'idps_cc_agno_previo', 'idps_hv_agno_previo',\n",
       "       'idps_pf_agno_previo', 'prom_idps_agno_previo', 'prom_lect_agno_previo',\n",
       "       'prom_mate_agno_previo', 'prom_mate_lect_agno_previo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_planificacion_implementacion_total.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69adbc01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agno\n",
      "rbd\n",
      "region\n",
      "deprov\n",
      "comuna\n",
      "dependencia\n",
      "nombre_actividad\n",
      "en_planificacion_anual\n",
      "dimensión_planif\n",
      "subdimensión_planif\n",
      "objetivo_planif\n",
      "estrategia_planif\n",
      "descripción_del_plan_planif\n",
      "fecha_inicio_planif\n",
      "fecha_término_planif\n",
      "programa_asociado_planif\n",
      "nombre_responsable_planif\n",
      "ate_planif\n",
      "planes_planif\n",
      "en_reporte_implementacion\n",
      "dimensión_implem\n",
      "subdimensión_implem\n",
      "objetivo_implem\n",
      "estrategia_implem\n",
      "descripción_del_plan_implem\n",
      "fecha_inicio_implem\n",
      "fecha_término_implem\n",
      "programa_asociado_implem\n",
      "nombre_responsable_implem\n",
      "ate_implem\n",
      "planes_implem\n",
      "monto_subvencion_general\n",
      "monto_sep\n",
      "monto_pie\n",
      "monto_mantenimiento\n",
      "monto_pro_retencion\n",
      "monto_internado\n",
      "monto_reforzamiento\n",
      "monto_feap\n",
      "monto_aporte_municipal\n",
      "monto_total\n",
      "nombre_nivel_ejecucion\n",
      "nombre_justificacion_nivel_ejecucion\n",
      "rural_rbd\n",
      "básica_adultos\n",
      "básica_niños\n",
      "media_hc_adultos\n",
      "media_hc_jóvenes\n",
      "media_tp_y_artística_adultos\n",
      "media_tp_y_artística_jóvenes\n",
      "parvularia\n",
      "parvularia_ind\n",
      "básica_niños_ind\n",
      "media_hc_jóvenes_ind\n",
      "media_tp_y_artística_jóvenes_ind\n",
      "básica_adultos_ind\n",
      "media_hc_adultos_ind\n",
      "media_tp_y_artística_adultos_ind\n",
      "total_niveles\n",
      "cod_total_niveles\n",
      "total_matricula\n",
      "asistencia\n",
      "prioritario_alu\n",
      "idps_am_agno_previo\n",
      "idps_cc_agno_previo\n",
      "idps_hv_agno_previo\n",
      "idps_pf_agno_previo\n",
      "prom_idps_agno_previo\n",
      "prom_lect_agno_previo\n",
      "prom_mate_agno_previo\n",
      "prom_mate_lect_agno_previo\n"
     ]
    }
   ],
   "source": [
    "for col in df_planificacion_implementacion_total.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc04351",
   "metadata": {},
   "source": [
    "# Text summarization\n",
    "\n",
    "https://huggingface.co/mrm8488/bert2bert_shared-spanish-finetuned-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fffd8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para analisis a nivel de PME (en lugar de acción)\n",
    "# Unir nombre de actividad y descripción \n",
    "df_planificacion_implementacion_total['nombre_y_descripción_actividad'] = df_planificacion_implementacion_total[['nombre_actividad', 'descripción_del_plan']].agg(': '.join, axis=1)\n",
    "\n",
    "df_planificacion_implementacion_total['nombre_y_descripción_actividad'] = df_planificacion_implementacion_total['nombre_y_descripción_actividad'].astype(str) + '\\n'\n",
    "\n",
    "# Agrupar por Objeetivo y estrategia \n",
    "grp = df_planificacion_implementacion_total.groupby(['rbd','objetivo', 'estrategia']).agg(C=('nombre_y_descripción_actividad', ', '.join)).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ckpt = 'mrm8488/bert2bert_shared-spanish-finetuned-summarization'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(ckpt)\n",
    "model = EncoderDecoderModel.from_pretrained(ckpt).to(device)\n",
    "\n",
    "def generate_summary(text):\n",
    " \n",
    "    inputs = tokenizer([text], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729efeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo\n",
    "text = 'día de la convivencia con la familia: se realizara una actividad de convivencia el día jueves 30 de mayo, en reunión de padres y apoderados, donde los estudiantes compartirán con sus familias, profesores y compañeros disfrutando de un espectáculo musical junto a la comunidad educativa, a cada nivel se les entregara 2 tortas grandes mas 2 bebidas de 3 lts. cada una para favorecer convivencia de cada nivel.\\n'\n",
    "generate_summary(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aafc30",
   "metadata": {},
   "source": [
    "# Sinónimos\n",
    "\n",
    "Permite calcular sinónimos de palabras clave PME, mediante dos procedimientos distintos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50782115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar categorizaciones pre establecidas \n",
    "keywords = pd.read_excel(path + r'\\Planes de Mejoramiento Educativo\\Listado palabras y conceptos Estudio PME FORMATEADO (Febrero).xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eebe334",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_dict = {}\n",
    "for ind, row in keywords.iterrows():\n",
    "    text = str(row['Palabras/Conceptos'])\n",
    "    text = text.replace('”', '\"')\n",
    "    text = text.replace('“', '\"')\n",
    "    print(ind)\n",
    "    keyword_dict[ind] = text.split('\"')[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fca833",
   "metadata": {},
   "source": [
    "## método 1: NLTK corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_sinonimos(palabra, idioma='spa'):\n",
    "    ''' Función que calcula las palabras sinónimas de una palabra dada\n",
    "    '''\n",
    "    sinonimos = []                                   # Conjunto de sinonimos inicialmente vacío\n",
    "    for syn in wn.synsets(palabra, lang=idioma):     # Para cada synset en el que aparezca la palabra\n",
    "        for lema in syn.lemmas(lang=idioma):         # Para cada lema de cada synset\n",
    "            sinonimos+=[lema.name()]               # Se añade el lema (en el atributo 'name') al conjunto\n",
    "    return sinonimos                                 # Se devuelve el conjunto de sinónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e046d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download language model: python -m spacy download es_core_news_sm \n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33827d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sinonimos = []\n",
    "keyterms = []\n",
    "for j in range(51):\n",
    "    for word in keyword_dict[j]:\n",
    "        print(word)\n",
    "        if len(word.split())==1:\n",
    "            doc = nlp(word)\n",
    "            lemmas = [token.lemma_ for token in doc]\n",
    "            for lemma in lemmas:\n",
    "                print(lemma)\n",
    "                keyterms += [lemma]\n",
    "                list_sinonimos += [[x for x in calcula_sinonimos(lemma) if lemma !=x and lemma.lower() !=x]]\n",
    "                print([x for x in calcula_sinonimos(lemma) if lemma !=x and lemma.lower() !=x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ee7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sinonimos = pd.DataFrame(list(zip(keyterms,list_sinonimos)),\n",
    "               columns =['Palabra clave', 'Sinónimos'])\n",
    "df_sinonimos['N_sinonimos'] = df_sinonimos['Sinónimos'].apply(len)\n",
    "df_sinonimos.sort_values(by='N_sinonimos',ascending=False,inplace=True)\n",
    "df_sinonimos.to_excel('sinonimos_palabras_clave.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a5caa4",
   "metadata": {},
   "source": [
    "## Método 2: WordReference scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinonimos_wordreference(word):\n",
    "    url='http://www.wordreference.com/sinonimos/'\n",
    "    buscar=url+word\n",
    "    resp=requests.get(buscar)\n",
    "    bs=BeautifulSoup(resp.text,'lxml')\n",
    "    lista=bs.find_all(class_='trans clickable')\n",
    "    for sin in lista:\n",
    "        sino=sin.find_all('li')\n",
    "    return sino[0].next_element.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac591053",
   "metadata": {},
   "source": [
    "# Sentence clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589bbdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_list = df_planificacion_total['Nombre Actividad'].unique()\n",
    "activities_list = [str(x) for x in activities_list]\n",
    "activities_list_test = activities_list[:1000]\n",
    "\n",
    "#look_for_clusters(eng_sentences, similarity_threshold=60)\n",
    "cluster_list = look_for_clusters(activities_list_test, language=\"spanish\", method=\"token_set_ratio\", similarity_threshold=90)\n",
    "\n",
    "# dataframe \n",
    "df_clusters = pd.DataFrame(list(zip(activities_list_test,cluster_list)),\n",
    "               columns =['Nombre_actividad', 'cluster'])\n",
    "\n",
    "for i in range(1,100):\n",
    "    print(df_clusters[df_clusters.cluster==i].Nombre_actividad.to_list(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7f8f0",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "https://medium.com/@jarch/topic-modeling-with-spanish-text-an-introductory-example-in-python-9cdb10fbe126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e207dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_planificacion_total[\"texto_total\"] = df_planificacion_total['nombre_actividad'].astype(str) + ' ' + df_planificacion_total['descripción_del_plan'].astype(str)\n",
    "# Spanish text analysis tools \n",
    "stopwords = set(nltk.corpus.stopwords.words(\"spanish\"))\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "def normalize_tokenize_lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    VALID_POS = {'PROPN', 'NOUN'}\n",
    "    # Avoid empty word in beggining with doc[1:]\n",
    "    return [\n",
    "        word.lemma_ for word in doc if word.lemma_ not in stopwords\n",
    "        and word.pos_ in VALID_POS\n",
    "        ]\n",
    "# This step may take a few minutes\n",
    "df_planificacion_total['texto_total_norm'] = df_planificacion_total['texto_total'].apply(normalize_tokenize_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a0156",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_activities = list(df_planificacion_total['nombre_actividad'].dropna().value_counts().index)[:50]\n",
    "lista_descripcion = list(df_planificacion_total[df_planificacion_total.nombre_actividad.isin(popular_activities)].descripción_del_plan.values)\n",
    "text_for_dict = [normalize_tokenize_lemmatize(str(x)) for x in lista_descripcion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedfe187",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(text_for_dict)\n",
    "# Assign ids to every word without gaps\n",
    "dictionary.compactify()\n",
    "# Filter out words that appear in less than 2 documents, thos appearing in more than 97% of corpus, and keep all other words\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.97, keep_n=None)\n",
    "# Reassign ids to every word without gaps after filtering\n",
    "dictionary.compactify()\n",
    "# For every review, create a B.O.W. representation\n",
    "corpus = [dictionary.doc2bow(text) for text in text_for_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b3c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal number of topics based on coherence\n",
    "df_planificacion_total['nombre_actividad'] = df_planificacion_total['nombre_actividad'].astype(str)\n",
    "texts = list(df_planificacion_total['nombre_actividad'].unique())[:1000]\n",
    "\n",
    "MAX_TOPICS = 20\n",
    "coherence_vals = []\n",
    "lm_list = []\n",
    "# This loop may take a while\n",
    "for n_topics in range(10, MAX_TOPICS+1):\n",
    "    print(n_topics)\n",
    "    lm = LdaModel(corpus=corpus, num_topics = n_topics,\n",
    "          id2word=dictionary)\n",
    "    lm_list.append(lm)\n",
    "    \n",
    "    cm = CoherenceModel(model=lm, texts=texts,\n",
    "         dictionary=dictionary, coherence='c_v')\n",
    "    \n",
    "    coherence_vals.append(cm.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fa4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(range(10, MAX_TOPICS+1), coherence_vals)\n",
    "ax.set_xlabel(\"N° de tópicos\")\n",
    "ax.set_ylabel(\"Coherencia\")\n",
    "ax.set_xticks(range(1,21))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c897f",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c370de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download language model: python -m spacy download es_core_news_sm \n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "#stopwords_esp = nltk.corpus.stopwords.words('spanish')               # Cargamos las palabras huecas en una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae7c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"Trayectoria\")\n",
    "texts = data['Nombre Actividad'].unique()\n",
    "texts = texts[:1000]\n",
    "\n",
    "# remove stopwords\n",
    "texts = [x.split() for x in texts]\n",
    "texts = [[y for y in x if y not in stopwords_esp] for x in texts]\n",
    "texts = [' '.join(x) for x in texts]\n",
    "\n",
    "i = 0\n",
    "list_sim = []\n",
    "for text in texts:\n",
    "    doc2 = nlp(text)\n",
    "    sim = doc1.similarity(doc2)\n",
    "    list_sim +=[[i,sim]]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b481690e",
   "metadata": {},
   "source": [
    "# PhraseMatcher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c12c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for searching keyword from the text\n",
    "def search_for_keyword(keyword, doc_obj, nlp):\n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "    phrase_list = [nlp(keyword)]\n",
    "    phrase_matcher.add(\"Text Extractor\", None, *phrase_list)\n",
    "\n",
    "    matched_items = phrase_matcher(doc_obj)\n",
    "\n",
    "    matched_text = []\n",
    "    for match_id, start, end in matched_items:\n",
    "        text = nlp.vocab.strings[match_id]\n",
    "        span = doc_obj[start: end]\n",
    "        matched_text.append(span.sent.text)\n",
    "    return matched_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d55c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = search_for_keyword('trayectorias', nlp('. '.join(texts)), nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd749221",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58915da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our Sentence Transformers model pre trained!!\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7286ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not production ready data!!\n",
    "sentences = [sentence.lower().replace('br','').replace('<',\"\").replace(\">\",\"\").replace('\\\\',\"\").replace('\\/',\"\") \n",
    "             for sentence in data['Descripción del Plan'].sample(n=2000)]\n",
    "\n",
    "#see a sentence, and our length\n",
    "print(sentences[5:6], f'\\n\\nLength Of Data {len(sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3636531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find the semantically closest sentence to a random sentence\n",
    "# that we come up with, in our dataset\n",
    "\n",
    "# i like action movies, mission impossible is one of my favorites\n",
    "our_sentence = 'Trayectoria'\n",
    "\n",
    "# lets embed our sentence\n",
    "my_embedding = model.encode(our_sentence)\n",
    "\n",
    "# lets embed the corpus\n",
    "#embeddings = model.encode(sentences)\n",
    "\n",
    "#Compute cosine similarity between my sentence, and each one in the corpus\n",
    "cos_sim = util.cos_sim(my_embedding, embeddings)\n",
    "\n",
    "# lets go through our array and find our best one!\n",
    "# remember, we want the highest value here (highest cosine similiarity)\n",
    "winners = []\n",
    "for arr in cos_sim:\n",
    "    for i, each_val in enumerate(arr):\n",
    "        winners.append([sentences[i],each_val])\n",
    "\n",
    "# lets get the top 2 sentences\n",
    "final_winners = sorted(winners, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "for arr in final_winners[0:2]:\n",
    "    print(f'\\nScore : \\n\\n  {arr[1]}')\n",
    "    print(f'\\nSentence : \\n\\n {arr[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df6b931",
   "metadata": {},
   "source": [
    "# SentenceTransformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c53735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar Sentence Transformers model preentrenado y apto para español\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a26f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar bbdd\n",
    "df_planificacion_total = pd.read_csv('planificacion_anual_parte1_2018_2022.csv', low_memory=False)\n",
    "# Frases a embeder\n",
    "sentences_descripcion_plan = df_planificacion_total['Descripción del Plan'].values\n",
    "# lets embed the corpus: 16 HORAS \n",
    "# embeddings = model.encode(sentences_descripcion_plan)\n",
    "# guardar \n",
    "# np.save(r'C:\\Users\\JoaquinFarina\\Desktop\\Tether\\embeddings_sentences_descripcion_plan.npy', embeddings)\n",
    "embeddings = np.load(r'C:\\Users\\JoaquinFarina\\Desktop\\Tether\\embeddings_sentences_descripcion_plan.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d937bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar categorizaciones pre establecidas \n",
    "data_cat = pd.read_excel(r'C:\\Users\\JoaquinFarina\\Desktop\\Tether\\Listado palabras y conceptos Esudio PME FORMATED.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a96e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences_descripcion_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar descripciones más cercanas a cada concepto\n",
    "lista_terminos = list(data_cat['Palabras/Conceptos'])\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('clasificacion_embedding_descripcion_actividad.xlsx', engine='xlsxwriter')\n",
    "\n",
    "for sentence in lista_terminos: \n",
    "    print('\\n',sentence)\n",
    "    sentence = sentence.replace('“', '')\n",
    "    sentence = sentence.replace('”', '')\n",
    "    sentence = sentence.replace(';', ',')\n",
    "    sentence = sentence.replace('*', ',')\n",
    "    sentence = sentence.replace(':', ',')\n",
    "    \n",
    "    # embeder concepto\n",
    "    my_embedding = model.encode(sentence)\n",
    "    # calcular similaridad de coseno entre concepto y corpus\n",
    "    cos_sim = util.cos_sim(my_embedding, embeddings)\n",
    "\n",
    "    # lets go through our array and find our best one!\n",
    "    # remember, we want the highest value here (highest cosine similiarity)\n",
    "    winners = []\n",
    "    for arr in cos_sim:\n",
    "        for i, each_val in enumerate(arr):\n",
    "            winners.append([sentences[i],each_val])\n",
    "\n",
    "    # lets get the top 2 sentences\n",
    "    final_winners = sorted(winners, key=lambda x: x[1], reverse=True)\n",
    "    print(final_winners[:2],'\\n')\n",
    "    \n",
    "    # Write each dataframe to a different worksheet.\n",
    "    pd.DataFrame(final_winners[:50000], columns = ['Descripción Actividad','score']) .to_excel(writer, sheet_name=sentence[:30])\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fadfec",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47645580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pandas version >= 0.21.0\n",
    "sheet_to_df_map = pd.read_excel('clasificacion_embedding_descripcion_actividad_annotated.xlsx', sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb45f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_nombres_cat = {\"Lenguaje y comunicación, Lengu\": \"Lenguaje y comunicación\",\n",
    "                    \"Inglés, idioma\": \"Inglés\",\n",
    "                    \"Matemática\": \"Matemática\",\n",
    "                    \"Ciencias Naturales, Biología, \" : \"Ciencias\",\n",
    "                    \"Historia, Geografía, Ciencias \": \"Historia\",\n",
    "                    \"Educación Física, salud, depor\": \"Educación Física\",\n",
    "                    \", Arte,, Música, musical, Visu\": \"Arte/Música\",\n",
    "                    \"Lengua y Cultura de los Pueblo\": \"Pueblos originarios\",\n",
    "                    \"ciudadanía digital, uso de tec\": \"Uso tecnología\",\n",
    "                    \"pandemia, crisis, sanitaria, C\": \"Pandemia\",\n",
    "                    \"convivencia, convivencia esco\" : \"Convivencia\",\n",
    "                    \"género, diversidad, diversidad\": \"Género/diversidad\"} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf65c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for cat in sheet_to_df_map: \n",
    "    df = sheet_to_df_map[cat]\n",
    "    if ('umbral' in df.columns) and ('migra' not in cat) :\n",
    "        cat2 = cat.replace('\"', '')\n",
    "        print(dict_nombres_cat[cat2])\n",
    "        \n",
    "        # procesar df \n",
    "        # identificar última fila a consideerar \n",
    "        i_umbral = df[df.umbral == 1].index[0]\n",
    "        # filtrar df\n",
    "        df_final = df[df.index<=i_umbral]\n",
    "        # eliminar nulos\n",
    "        df_final.dropna(subset = ['Descripción Actividad'],inplace=True)\n",
    "        # eliminar duplicados\n",
    "        df_final.drop_duplicates(subset = ['Descripción Actividad'],inplace=True)\n",
    "        # definir columna de conteo \n",
    "        df_final.loc[:,'CAT ' + dict_nombres_cat[cat2]] = 1\n",
    "        # acotar columnas\n",
    "        df_final = df_final[['Descripción Actividad','CAT ' + dict_nombres_cat[cat2]]]\n",
    "        # homologar nombre columna\n",
    "        df_final.rename(columns = {'Descripción Actividad':'Descripción del Plan'}, inplace=True)\n",
    "        \n",
    "        if j == 0:\n",
    "            df_planificacion_total_CAT = pd.merge(df_planificacion_total,df_final,  on = 'Descripción del Plan', how = 'left')\n",
    "        else:\n",
    "            df_planificacion_total_CAT = pd.merge(df_planificacion_total_CAT,df_final,  on = 'Descripción del Plan', how = 'left')\n",
    "        j+=1\n",
    "        \n",
    "        print(len(df_final))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e376b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def columnas_agregacion(x):\n",
    "    d = {}\n",
    "    d['Actividades por EE'] = np.round(x['rbd'].count()/x['rbd'].nunique(),1)\n",
    "    return pd.Series(d, index=['Actividades por EE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74762062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_actividades_por_EE_variable(variable):\n",
    "    for a in range(2018,2023):\n",
    "        if a == 2018:\n",
    "            df_tot = df_planificacion_total_CAT[df_planificacion_total_CAT.agno == a][[variable,'rbd']].groupby([variable]).apply(columnas_agregacion)\n",
    "            df_tot.columns = [str(a) for x in df_tot.columns]\n",
    "        else:\n",
    "            s2 = df_planificacion_total_CAT[df_planificacion_total_CAT.agno == a][[variable,'rbd']].groupby([variable]).apply(columnas_agregacion)\n",
    "            s2.columns = [str(a) for x in s2.columns]\n",
    "            df_tot = pd.concat([df_tot,s2], axis = 1)\n",
    "    return df_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_total_EE = []\n",
    "list_total_prom = [] \n",
    "for col in df_planificacion_total_CAT.columns: \n",
    "    if 'CAT' in col:\n",
    "        print(col)\n",
    "        list_cat_EE = []\n",
    "        list_cat_prom = []\n",
    "        for a in range(2018,2023):\n",
    "            df_agno = df_planificacion_total_CAT[df_planificacion_total_CAT.agno == a]\n",
    "            # Actividades por EE\n",
    "            N_EE = len(df_agno[df_agno[col] == 1])/len(df_agno.rbd.unique()) \n",
    "            # promedio de actividades \n",
    "            N_prom = len(df_agno[df_agno[col] == 1])/len(df_agno) \n",
    "            \n",
    "            list_cat_EE += [np.round(N_EE,2)]\n",
    "            list_cat_prom += [100*np.round(N_prom,3)]\n",
    "            \n",
    "        list_total_EE += [[col[4:]] + list_cat_EE]\n",
    "        list_total_prom += [[col[4:]] + list_cat_prom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cccbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_EE = pd.DataFrame(list_total_EE, columns = ['Categoría', 2018,2019,2020,2021,2022])\n",
    "df_total_EE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_prom = pd.DataFrame(list_total_prom, columns = ['Categoría', 2018,2019,2020,2021,2022])\n",
    "df_total_prom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter(r'C:\\Users\\JoaquinFarina\\Dropbox\\Team NLP\\Data\\Planes de Mejoramiento Educativo\\Informe1\\Tablas_informe1\\tablas_categorzaciones.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Write each dataframe to a different worksheet.\n",
    "df_total_EE.to_excel(writer, sheet_name='actividades por EE')\n",
    "df_total_prom.to_excel(writer, sheet_name='% de actividades')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_planificacion_total_CAT['CAT_suma'] = 0\n",
    "for col in df_planificacion_total_CAT.columns: \n",
    "    if ('CAT' in col) and ('suma' not in col):\n",
    "        print(col)\n",
    "        df_planificacion_total_CAT['CAT_suma'] = df_planificacion_total_CAT['CAT_suma'] + df_planificacion_total_CAT[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d83308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(2018,2023):\n",
    "    df_a = df_planificacion_total_CAT[df_planificacion_total_CAT.agno==a]\n",
    "    print(a)\n",
    "    if a == 2018:\n",
    "        df_resumen_suma_act = np.round(100*df_a.CAT_suma.value_counts().sort_index()/len(df_a),2)\n",
    "    else:\n",
    "        df_resumen_suma_act = pd.concat([df_resumen_suma_act,np.round(100*df_a.CAT_suma.value_counts().sort_index()/len(df_a),2) ],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e603262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resumen_suma_act.columns = [2018,2019,2020,2021,2022]\n",
    "df_resumen_suma_act.to_excel(r'Informe1\\Tablas_informe1\\df_resumen_suma_actividades.xlsx')\n",
    "df_resumen_suma_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cca05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_planificacion_total_CAT.to_csv(r'planificacion_anual_parte1_2018_2022_EDA_classification_FINAL.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
